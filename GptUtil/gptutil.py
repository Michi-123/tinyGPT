# -*- coding: utf-8 -*-
"""GptUtil.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fi2vSIEM7wVjSrghdecjsbwDMxv5OiEd
"""

import torch
from torch.utils.data import Dataset, DataLoader
# from torch.nn.utils.rnn import pad_sequence

# @title TranslationDataset
class TranslationDataset(Dataset):
    def __init__(self, pairs, max_sequence_length):
        self.pairs = pairs
        self.max_sequence_length = max_sequence_length
        self.source_vocab, self.target_vocab = self.build_vocab()

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        source_text, target_text = self.pairs[idx] # 'Hello how are you?',  'こんにち は 、 お 元気 です か ？'

        source_tokens = self.tokenize(source_text) # ['Hello', 'how', 'are', 'you?']
        target_tokens = self.tokenize(target_text)

        padded_source_tokens = self.pad_sequence(source_tokens, self.max_sequence_length) # ['Hello', 'how', 'are', 'you?', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']
        padded_target_tokens = self.pad_sequence(target_tokens, self.max_sequence_length)

        source_indices = self.tokens_to_indices(padded_source_tokens, self.source_vocab)
        target_indices = self.tokens_to_indices(padded_target_tokens, self.target_vocab)

        return {
            'source_indices': torch.tensor(source_indices),
            'target_indices': torch.tensor(target_indices),
        }

    def build_vocab(self):
        source_texts, target_texts = zip(*self.pairs)

        source_tokens = [token for text in source_texts for token in self.tokenize(text)]
        target_tokens = [token for text in target_texts for token in self.tokenize(text)]

        source_unique_tokens = set(source_tokens + ['[PAD]'])  # [PAD] を追加
        target_unique_tokens = set(target_tokens + ['[PAD]'])  # [PAD] を追加

        source_vocab = {token: idx for idx, token in enumerate(source_unique_tokens)}
        target_vocab = {token: idx for idx, token in enumerate(target_unique_tokens)}

        return source_vocab, target_vocab

    def tokenize(self, text):
        # Simple tokenization, split by spaces
        return text.split()

    def pad_sequence(self, tokens, max_length):
        if len(tokens) < max_length:
            padding = ['[PAD]'] * (max_length - len(tokens))
            tokens += padding
        else:
            tokens = tokens[:max_length]
        return tokens

    def tokens_to_indices(self, tokens, vocab):
        return [vocab[token] for token in tokens]

#@title TextDataset
class TextDataset(Dataset):
    def __init__(self, corpus, max_sequence_length):
        self.corpus = corpus
        self.max_sequence_length = max_sequence_length
        self.source_vocab = self.build_vocab()
        vocab = set(word for sentence in corpus for word in sentence.split())
        self.word_to_index = {word: idx for idx, word in enumerate(vocab)}

    def __len__(self):
        return len(self.corpus)

    def __getitem__(self, idx):
        source_text = self.corpus[idx]
        source_tokens = self.tokenize(source_text)
        padded_source_tokens = self.pad_sequence(source_tokens, self.max_sequence_length)
        source_indices = self.tokens_to_indices(padded_source_tokens, self.source_vocab)
        return {
            'source_indices': torch.tensor(source_indices),
        }

    def build_vocab(self):
        source_texts = self.corpus
        source_tokens = [token for text in source_texts for token in self.tokenize(text)]
        source_unique_tokens = set(source_tokens + ['[PAD]'])  # [PAD] を追加
        source_vocab = {token: idx for idx, token in enumerate(source_unique_tokens)}
        return source_vocab

    def tokenize(self, text):
        # Simple tokenization, split by spaces
        return text.split()

    def pad_sequence(self, tokens, max_length):
        if len(tokens) < max_length:
            padding = ['[PAD]'] * (max_length - len(tokens))
            tokens += padding
        else:
            tokens = tokens[:max_length]
        return tokens

    def tokens_to_indices(self, tokens, vocab):
        return [vocab[token] for token in tokens]

import re
import unicodedata

#@title PrepareData
class PrepareData:

    def __init__(self, tagger, data_path='/content/data/eng-jpn.txt', max_length=15, use_filterPairs=True):

        self.tagger = tagger
        self.data_path = data_path
        self.max_length = max_length
        self.use_filterPairs = use_filterPairs

        self.reverse = True
        self.eng_prefixes = (
            "i am ", "i m ",
            "he is", "he s ",
            "she is", "she s ",
            "you are", "you re ",
            "we are", "we re ",
            "they are", "they re "
        )

    def translation_data(self):
        print("読み込み中...")

        lines = open(self.data_path, encoding='utf-8').read().strip().split('\n')

        pairs = []
        for line in lines:

            pair = [] # 英語：日本語
            word = line.split('\t')
            pair.append(self._normalizeString(word[0]))

            # フランス・スペイン・ドイツ語対応
            # s2.append(self._normalizeString(s[1]))

            # 日本語対応
            watkati_gaki = self.tagger.parse(word[1]);
            watkati_gaki = watkati_gaki[:-1]
            pair.append(watkati_gaki)

            pairs.append(pair)

        if self.reverse:
            pairs = [list(reversed(p)) for p in pairs]

        # メモリー節約のために限定学習
        if self.use_filterPairs :pairs = self._filterPairs(pairs)

        print("%s個の文章のペアを読み込みます" % len(pairs))

        return pairs


    def gpt_data(self):
        pairs = self.translation_data()

        corpus = []
        for pair in pairs:
            corpus.append(pair[0])
            corpus.append(pair[1])

        return corpus

    def _normalizeString(self, s):
        s = self._unicodeToAscii(s.lower().strip())
        s = re.sub(r"([.!?])", r" \1", s)
        s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
        return s

    def _unicodeToAscii(self, s):
        return ''.join(
            c for c in unicodedata.normalize('NFD', s)
            if unicodedata.category(c) != 'Mn'
        )

    def _filterPairs(self, pairs):
        return [pair for pair in pairs if self._filterPair(pair)]

    def _filterPair(self, p):
        return len(p[0].split(' ')) < self.max_length and \
            len(p[1].split(' ')) < self.max_length and \
            p[1].startswith(self.eng_prefixes)

"""# Debug"""

#@title 学習用データセット
!wget https://www.manythings.org/anki/jpn-eng.zip -P data
!unzip -o data/jpn-eng.zip -d data
!mv data/jpn.txt data/eng-jpn.txt
# テキストデータの読み込み
rawdata = open('data/%s-%s.txt' % ("eng", "jpn"), encoding='utf-8')
lines = rawdata.read().strip().split('\n')
len(lines)

# @title MeCab
!pip install mecab-python3==1.0.8
# MeCabの補足プログラム
!pip install unidic-lite

import MeCab
tagger = MeCab.Tagger("-Owakati")

prepareData = PrepareData(tagger)
pairs = prepareData.translation_data()

print(len(pairs))
pairs[1000:1010]

corpus = prepareData.gpt_data()
corpus[:10]

